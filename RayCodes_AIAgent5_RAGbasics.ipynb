{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEe9RhMUKPb0"
      },
      "source": [
        "# Library Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83jufhd0-wfG",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "666e51de-cb7f-4475-e85d-14fa30e654eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m420.4/420.4 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.2/56.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.3/72.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m388.2/388.2 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m376.8/376.8 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m228.7/228.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m442.8/442.8 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.0/95.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q pydantic-ai pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9SIPphM7X0Cr"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "from pydantic_ai.providers.google import GoogleProvider\n",
        "from pydantic_ai.models.google import GoogleModel\n",
        "from pydantic_ai import Agent\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import fitz\n",
        "\n",
        "import numpy as np\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def nbprint(s):\n",
        "    display(Markdown(s))"
      ],
      "metadata": {
        "id": "Etg91LEyKmYA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Gemini model via PydanticAI\n"
      ],
      "metadata": {
        "id": "S7_2vd9ZJ0bB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GEMINI_MODEL\"]  = \"gemini-2.5-flash-lite\"\n",
        "os.environ[\"GEMINI_KEY\"] = userdata.get('GeminiKey')"
      ],
      "metadata": {
        "id": "ZiJAG5ONKSm5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=os.environ[\"GEMINI_KEY\"])\n",
        "\n",
        "provider = GoogleProvider(api_key=os.environ[\"GEMINI_KEY\"])\n",
        "model = GoogleModel(os.environ[\"GEMINI_MODEL\"], provider=provider)\n",
        "rag_agent = Agent(model=model,\n",
        "                  system_prompt=\"You are a RAG assistant. Use ONLY the context to answer clearly.\")\n",
        "\n",
        "response = await rag_agent.run(\"\"\"World ended in 2012 and now it is 2025.\n",
        " How many years have passed since World ended?\"\"\")\n",
        "nbprint(response.output)\n"
      ],
      "metadata": {
        "id": "es34d6HnJzpx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "8bca4981-7f7e-4097-cab4-d074ac34697f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The world ended in 2012. The current year is 2025.\nTherefore, 2025 - 2012 = 13 years have passed since the world ended."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“˜ Data Store Preparation Pipeline\n",
        "\n",
        "- ğŸ— **Download PDF** â†’ Get the source document  \n",
        "- ğŸ“„ **Extract Text** â†’ Convert PDF into raw text  \n",
        "- âœ‚ï¸ **Chunk Text** â†’ Break text into smaller pieces  \n",
        "- ğŸ”¢ **Generate Embeddings** â†’ Represent chunks as vectors  \n",
        "- ğŸ“¦ **Prepare Vector Store** â†’ Store embeddings for retrieval  \n"
      ],
      "metadata": {
        "id": "y2yfbyQkok2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch + Parse PDF"
      ],
      "metadata": {
        "id": "46PaAbgnJ1xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://arxiv.org/pdf/2510.25137.pdf\"\n",
        "pdf_bytes = requests.get(url).content\n",
        "\n",
        "with open(\"temp.pdf\", \"wb\") as f:\n",
        "    f.write(pdf_bytes)\n",
        "\n",
        "doc = fitz.open(\"temp.pdf\")\n",
        "\n",
        "full_text = \"\"\n",
        "for page in doc:\n",
        "    full_text += page.get_text(\"text\") + \"\\n\"\n",
        "\n",
        "nbprint(full_text[:100])\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "t_41hmufgF8n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "455f665c-8ca3-4379-d7fe-dba4e2205f38"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "PROJECT ICEBERG\nThe Iceberg Index:\nMeasuring Workforce Exposure in the\nAI Economy\nAyush Chopra1,6\nSa"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking"
      ],
      "metadata": {
        "id": "jghELTuKKCXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, max_chars=1000, overlap=5):\n",
        "    chunks = []\n",
        "    for i in range(0, len(text), max_chars):\n",
        "      start = max(0, i-overlap)\n",
        "      end = min(len(text), i+max_chars+overlap)\n",
        "      chunks.append(text[start:end])\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(full_text, max_chars=1000)\n",
        "print(len(chunks))\n",
        "nbprint(chunks[0])"
      ],
      "metadata": {
        "id": "v9BbGwBjKCC9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "8378af8b-8d04-4318-fe8f-767d18ebaacd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "PROJECT ICEBERG\nThe Iceberg Index:\nMeasuring Workforce Exposure in the\nAI Economy\nAyush Chopra1,6\nSantanu Bhattacharya1,6\nDeAndrea Salvador3,4,1\nAyan Paul6\nTeddy\nWright5\nAditi Garg6\nFeroz Ahmad6\nAlice C. Schwarze5\nRamesh Raskar1,6\nPrasanna Balaprakash2\n1Massachusetts Institute of Technology\n2Oak Ridge National Laboratory\n3North Carolina General Assembly\n4National Task Force on State AI Policy\n5Utah Office of AI Policy, Dept of Commerce\n6Project Iceberg\nAbstract. Artificial Intelligence is reshaping Americaâ€™s over $9.4 trillion labor market, with\ncascading effects that extend far beyond visible technology sectors. When AI automates\nquality control in automotive plants, consequences spread through logistics networks,\nsupply chains, and local service economies. Yet traditional workforce metrics cannot\ncapture these ripple effects: they measure employment outcomes after disruption occurs,\nnot where AI capabilities overlap with human skills before adoption crystallizes. Project\nIceberg addresses"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding and Vector Store"
      ],
      "metadata": {
        "id": "h73oj6vUKJxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed(text):\n",
        "    resp = genai.embed_content(model=\"text-embedding-004\",\n",
        "                               content=text,\n",
        "                               task_type=\"retrieval_document\")\n",
        "    return np.array(resp[\"embedding\"], dtype=float)\n",
        "\n",
        "embeddings = np.array([embed(c) for c in chunks])\n",
        "print(len(embeddings))\n",
        "print(embeddings[0][:5])\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KaxsCEyBKB9A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "36fe3e48-95cc-4481-ab0f-222455ac13e4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "[ 0.03459131 -0.00489019 -0.05584043  0.05050938  0.0224702 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ’¬ User Query Answering Pipeline\n",
        "\n",
        "- ğŸ™‹ **User Query** â†’ Input from the user  \n",
        "- ğŸ” **Embed Query** â†’ Convert query into vector  \n",
        "- ğŸ“‚ **Search Vector Store** â†’ Find closest matching chunks  \n",
        "- ğŸ“‘ **Retrieve Chunks** â†’ Get relevant context  \n",
        "- ğŸ§  **Generate Answer** â†’ Formulate final response  \n"
      ],
      "metadata": {
        "id": "g_DKgF5voqKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_queries = [\n",
        "    \"What is the Iceberg Index?\",\n",
        "    \"What problem does Project Iceberg aim to solve?\",\n",
        "    \"How many workers does Project Iceberg simulate?\",\n",
        "    \"What are the three main steps of Project Icebergâ€™s simulation process?\",\n",
        "    \"Does the Iceberg Index measure job loss or technical exposure?\",\n",
        "    \"What percentage of labor market wage value is represented by current visible AI adoption (Surface Index)?\",\n",
        "    \"What percentage is represented by the larger hidden exposure (Iceberg Index)?\",\n",
        "    \"How many AI tools were cataloged for the framework?\",\n",
        "    \"How many occupations are modeled in the simulation?\",\n",
        "    \"Which state has the highest Surface Index according to the paper?\",\n",
        "    \"What is meant by 'automation surprise'?\",\n",
        "    \"What kinds of tasks does the Surface Index primarily capture?\",\n",
        "    \"What kinds of tasks does the Iceberg Index primarily capture?\",\n",
        "    \"Does the Iceberg Index predict job displacement? Why or why not?\",\n",
        "    \"What is the role of wage-value weighting in the Iceberg Index?\",\n",
        "    \"What does the validation against career transition data show?\",\n",
        "    \"What is the agreement percentage between the Surface Index and the Anthropic Economic Index?\",\n",
        "    \"Which traditional economic metrics fail to capture Iceberg exposure effectively?\",\n",
        "    \"Which states show the largest gap between Surface Index and Iceberg Index?\",\n",
        "    \"What does the Herfindahlâ€“Hirschman Index (HHI) reveal about exposure concentration?\"\n",
        "]"
      ],
      "metadata": {
        "id": "UJKmX_lRot3X"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval"
      ],
      "metadata": {
        "id": "n69c_gH0LVfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_context(question, top_k=3):\n",
        "    q_emb = embed(question)\n",
        "    sims = embeddings.dot(q_emb) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(q_emb))\n",
        "    sorted_sims_idxes = np.argsort(sims)[::-1]\n",
        "    idxs = sorted_sims_idxes[:top_k]\n",
        "    return \"\\n\\n\".join(chunks[i] for i in idxs)"
      ],
      "metadata": {
        "id": "UwgD6-U4KB4A"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augmentation"
      ],
      "metadata": {
        "id": "rv0ogF2fLYGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment(query, verbose=False):\n",
        "  context = retrieve_context(query, top_k=3)\n",
        "  if verbose:\n",
        "    nbprint('---')\n",
        "    nbprint(context)\n",
        "    nbprint('---')\n",
        "\n",
        "  aug_prompt = f\"Context:\\n {context} \\n \\n Question: {query} \\n Answer based only on the context.\"\n",
        "  return aug_prompt"
      ],
      "metadata": {
        "id": "JlsAHb-BQR_5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def answer_with_agent(prompt):\n",
        "    res = await rag_agent.run(prompt)\n",
        "    return res.output"
      ],
      "metadata": {
        "id": "tIVkLnVLJ9wA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usage"
      ],
      "metadata": {
        "id": "8_X9VvV_LeRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = user_queries[3]\n",
        "aug_prompt = augment(query, verbose=True)\n",
        "ans = await answer_with_agent(aug_prompt)\n",
        "nbprint(f\" **Q:** {query} \\n\\n **A:** {ans} \\n\\n ---\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2E_yTHRkLh5Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "e7bb8d47-3139-488e-e648-e93fb0093052"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "g systems, provides\nthe computational execution layer. The infrastructure delivers massively parallel processing across\n20\n\nProject Iceberg\nthousands of compute nodes with GPU acceleration that supports large-scale optimization and\nanalysis operations. This computational capacity transforms previously intractable population\nmodeling challenges into feasible real-time policy analysis tools. These integrated components\nprovide the computational foundation for the Iceberg Index and its policy scenario testing\nfunctionality, enabling dynamic analysis of workforce transformation scenarios under different\npolicy conditions.\n21\n\n\n\nures,\nor regulatory choicesâ€”and run scenarios under different assumptions of technology maturity and\nadoption. The simulation then yields three kinds of insight: how occupations and skills evolve\nover time, where disruption is geographically spread, and how shocks in one sector cascade into\nothers. These outputs allow policymakers to compare strategies side by side and anticipate both\ndirect and indirect consequences.\nThe framework builds on validated foundations (see Appendix A), ensuring that the simulated\ndynamics reflect real labor-market structure. Its primary output is the Iceberg Indexâ€”a skills-\ncentered measure of workforce transformation introduced in Section 4.\nProject Iceberg Goal: To provide a sandbox for testing workforce strategies before\nimplementationâ€”enabling policymakers to assess technical exposure, evaluate\ninterventions under different scenarios, and target preparation where it matters most.\n5\n\nProject Iceberg\n4\nThe Iceberg Index\nThe Iceberg Index is a skills-centered measur\n\nesigning pathways when jobs shift.\nStep 2 â€“ AI Workforce Mapping: We catalog over 13,000 AI tools - including copilots\nand workflow automation systems - using the same skill taxonomy. This allows direct comparison\nbetween human and AI capabilities. The alignment reveals where AI augments human work\n(e.g. automating hospital paperwork so nurses can spend more time with patients) and where it\ntransforms tasks entirely (e.g. accelerating code generation requires engineers to shift towards\noversight, testing, and integration skills).\nStep 3 - Humanâ€“AI Simulation: We combine these two populations in Large Population\nModels (LPMs), simulating billions of interactions between workers, skills, and AI tools. These\nsimulations incorporate factors such as technology readiness, adoption behavior, and regional\nvariation. Policymakers can use the resulting scenarios to anticipate disruption, test reskilling\n4\n\nProject Iceberg\nFigure 3. The Iceberg Index reveals workforce exposure five times larger than visibl"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " **Q:** What are the three main steps of Project Icebergâ€™s simulation process? \n\n **A:** The three main steps of Project Iceberg's simulation process are:\n\n1.  **AI Workforce Mapping**: Cataloging over 13,000 AI tools using a skill taxonomy for direct comparison with human capabilities.\n2.  **Humanâ€“AI Simulation**: Combining workers and AI tools in Large Population Models (LPMs) to simulate interactions, incorporating factors like technology readiness, adoption behavior, and regional variation.\n3.  **Scenario Testing**: Using the resulting scenarios to anticipate disruption and test reskilling, or regulatory choices under different assumptions of technology maturity and adoption. \n\n ---"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Meqrf2VP2GAd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6hCxEqTrxBFevf+N5jEPa"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}